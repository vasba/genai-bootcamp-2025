version: '3'
services:
  llm:
    image: opea/llm-textgen:latest
    environment:
      - MODEL_ID=${LLM_MODEL_ID:-meta-llama/Llama-2-7b-chat}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
    ports:
      - "8008:80"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  embedding:
    image: opea/embedding:latest
    ports:
      - "8001:80"

  retriever:
    image: opea/retriever:latest
    ports:
      - "7000:7000"
    volumes:
      - ./data:/app/data

  expertqa:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8888:8888"
    environment:
      - EMBEDDING_SERVER_HOST_IP=embedding
      - EMBEDDING_SERVER_PORT=80
      - RETRIEVER_SERVICE_HOST_IP=retriever
      - RETRIEVER_SERVICE_PORT=7000
      - LLM_SERVER_HOST_IP=llm
      - LLM_SERVER_PORT=80
    depends_on:
      - llm
      - embedding
      - retriever

  nginx:
    image: opea/nginx:latest
    ports:
      - "${NGINX_PORT:-80}:80"
    depends_on:
      - expertqa